{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "import os\n",
    "import struct\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pathlib\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "from scipy import fftpack\n",
    "import math\n",
    "from matplotlib import patches\n",
    "import matplotlib.ticker as mtick\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(file_dir):   \n",
    "    pickle_L=[]   \n",
    "    pickle_name=[]\n",
    "    for dirpath, dirnames, filenames in os.walk(file_dir):  \n",
    "        for file in filenames :  \n",
    "            if os.path.splitext(file)[1] == '.pickle':  \n",
    "                pickle_L.append(os.path.join(dirpath, file))  \n",
    "                pickle_name.append(os.path.join(file))  \n",
    "            \n",
    "    #return L \n",
    "    return  pickle_name\n",
    "    #return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleLoader(pklFile):\n",
    "    try:\n",
    "        while True:\n",
    "            yield pickle.load(pklFile)\n",
    "    except EOFError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def time(nums):\n",
    "    start=[]\n",
    "    end=[]\n",
    "    arr=[]\n",
    "    for k, g in groupby(enumerate(nums), lambda ix : ix[0] - ix[1]):\n",
    "        arr.append(list(map(itemgetter(1), g)))\n",
    "    for j in range (0,len(arr)):\n",
    "        if (len(arr[j])>=2):\n",
    "            start.append(arr[j][0])\n",
    "            end.append(arr[j][-1])\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSmallest(arr):\n",
    "    smallest = arr[0]#将第一个元素的值作为最小值赋给smallest    \n",
    "    smallest_index = 0#将第一个值的索引作为最小值的索引赋给smallest_index    \n",
    "    for i in range(1, len(arr)):        \n",
    "        if arr[i] < smallest:#对列表arr中的元素进行一一对比            \n",
    "            smallest = arr[i]            \n",
    "            smallest_index = i    \n",
    "    return smallest_index\n",
    "def selectionSort(arr):    \n",
    "    newArr = []    \n",
    "    for i in range(len(arr)):        \n",
    "        smallest = findSmallest(arr)#一共要调用5次findSmallest        \n",
    "        newArr.append(arr.pop(smallest))#每一次都把findSmallest里面的最小值删除并存放在新的数组newArr中    \n",
    "    return newArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#load model\n",
    "filename='JNE_XG_W478.sav'\n",
    "\n",
    "rf=joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_path='Epi-AI_processed_files/Test/Model_I/'\n",
    "#tra_path='Epi-AI_processed_files/Test/Model_II/'\n",
    "#tra_path='Epi-AI_processed_files/Test/Model_III/'\n",
    "#tra_path='Epi-AI_processed_files/Test/Model_IV/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=read_pickle(tra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepcificity=[]\n",
    "sensitivity=[]\n",
    "accuracy=[]\n",
    "ROC=[]\n",
    "file_name=[]\n",
    "\n",
    "\n",
    "for i in range (0, len(train_file)):\n",
    "    with open(tra_path+train_file[i], 'rb') as file:\n",
    "        #print(train_file[i])\n",
    "        file_name.append(train_file[i])\n",
    "        for event in pickleLoader(file):\n",
    "            #print(event)\n",
    "            data_test=event[['target','mean_EEG2', 'EEG2_mobility', 'TKEO_EEG2', 'EEG2_P_delta','EEG2_P_theta',\n",
    "                                'EEG2_P_alpha','EEG2_P_beta','EEG2_P_gamma','EEG2_P_total','EEG2_rel_delta','EEG2_rel_theta',\n",
    "                                'EEG2_rel_alpha','EEG2_rel_beta','EEG2_rel_gamma','EEG2_pfd','EEG2_skew',\n",
    "                                'EEG2_kurtosis','EEG2_var','EEG2_envelope']]\n",
    "            \n",
    "            data_test.columns=['target','mean', 'mobility','TKEO', 'absolute delta power','absolute theta power',\n",
    "               'absolute alpha power','absolute beta power','absolute gamma power','absolute total power',\n",
    "               'relative delta power','relative theta power','relative alpha power','relative beta power',\n",
    "               'relative gamma power','fractal dimension','skewness',\n",
    "                'kurtosis','variance','signal envelope']\n",
    "     \n",
    "            data_test=data_test.reset_index(drop=True)\n",
    "            \n",
    "            train_X = data_test.drop('target', axis=1)\n",
    "            train_y=data_test.target\n",
    "\n",
    "            manual=train_y.where(train_y==1)\n",
    "            manual=manual.dropna()\n",
    "            manunum=len(time(manual.index)[0])\n",
    "\n",
    "            predicted_proba = rf.predict_proba(train_X)\n",
    "            train_pred= (predicted_proba [:,1] >= threshold).astype('int')\n",
    "            \n",
    "            train_pred=pd.DataFrame(train_pred)\n",
    "  \n",
    "            pred=train_pred\n",
    "            auto=pred.where(pred==1)\n",
    "            auto=auto.dropna()\n",
    "            val=time(auto.index)\n",
    "            start_ti=[-1]\n",
    "            end_ti=[-1]\n",
    "        \n",
    "            \n",
    "            for jjj in range(0, len(val[0])):\n",
    "                start_ti.append(val[0][jjj])\n",
    "                end_ti.append(val[1][jjj])\n",
    "           \n",
    "            start_ti=pd.DataFrame(start_ti)\n",
    "            end_ti=pd.DataFrame(end_ti)\n",
    "\n",
    "            frame=[start_ti,end_ti]\n",
    "            performance=pd.concat(frame,axis=1)\n",
    "            performance.columns=['start_time','end_time']\n",
    "            \n",
    "            spindle_start=start_ti\n",
    "            spindle_end=end_ti\n",
    "    \n",
    "            spindle_start=pd.DataFrame(spindle_start)\n",
    "            spindle_end=pd.DataFrame(spindle_end)\n",
    "            spindle_dur=spindle_end-spindle_start\n",
    "            \n",
    "            frame0=[spindle_start,spindle_end,spindle_dur]\n",
    "            spindle_time=pd.concat(frame0,axis=1)\n",
    "            spindle_time.columns=['spindle_start','spindle_end','spindle_dur']\n",
    "            spindle_time= spindle_time.drop_duplicates(subset=['spindle_end'], keep='first')\n",
    "            spindle_time = spindle_time[spindle_time['spindle_dur'] >=2]\n",
    "            #print(len(spindle_time))\n",
    "            #print(len(spindle_time))\n",
    "            spindle_time=spindle_time.reset_index(drop=True)\n",
    "            \n",
    "            #ppp='D:/desk/EEG/UCC_infant/remove_abmornal/auto_label/'\n",
    "            #pathlib.Path(ppp).mkdir(parents=True, exist_ok=True) \n",
    "            \n",
    "            #spindle_time.to_csv(ppp+'time_'+train_file[i]+'.csv')\n",
    "            \n",
    "            start_com=pd.DataFrame(start_ti)\n",
    "            end_com=pd.DataFrame(end_ti)\n",
    "            start_com=(start_com).astype(int)\n",
    "            end_com=(end_com).astype(int)+1\n",
    "            start_com=pd.DataFrame(start_com)\n",
    "            end_com=pd.DataFrame(end_com)\n",
    "            dur_com=end_com-start_com\n",
    "            \n",
    "            frame=[start_com,end_com,dur_com]\n",
    "            time_ta=pd.concat(frame,axis=1)\n",
    "            time_ta.columns=['start_com','end_com','dur_com']\n",
    "            time_table= time_ta.drop_duplicates(subset=['end_com'], keep='first')\n",
    "            #print(len(time_table))\n",
    "            time_table = time_table[time_table['dur_com'] >=2]\n",
    "            #print(len(time_table))\n",
    "            time_table=time_table.reset_index(drop=True)\n",
    "            \n",
    "            autonum=len(time_table)\n",
    "            \n",
    "            spindle_time['Start_signal']=(spindle_time['spindle_start'])\n",
    "            spindle_time['End_signal']=(spindle_time['spindle_end'])\n",
    "            spindle_time['Duration']=spindle_time['End_signal']-spindle_time['Start_signal']\n",
    "\n",
    "            data_test['tkeo_label']=0\n",
    "            for i in range(0,len(spindle_time)):\n",
    "                data_test['tkeo_label'][spindle_time['Start_signal'].values[i]-3:spindle_time['End_signal'].values[i]+3]=1\n",
    "                \n",
    "           \n",
    "            ind=[-5]\n",
    "            index=[-6]\n",
    "\n",
    "            for iiii in range(0, len(train_y)):\n",
    "                if ( data_test['tkeo_label'][iiii]==1 & train_y[iiii]==1):\n",
    "                    ind.append(iiii)\n",
    "            \n",
    "            #print(ind[:200])\n",
    "            index.append(ind[0])    \n",
    "            for jjjj in range (1, len(ind)):\n",
    "                if ind[jjjj]-ind[jjjj-1]!=1:\n",
    "                    index.append(ind[jjjj])\n",
    "            \n",
    "            tn, fp, fn, tp = confusion_matrix(train_y, data_test['tkeo_label'], labels=[0,1]).ravel()\n",
    "            spec = tn / (tn+fp)\n",
    "            sens=tp/(tp+fn)\n",
    "            falsepr=fp/(fp+tn)\n",
    "            falsedr=fp/(fp+tp)\n",
    "           \n",
    "            sepcificity.append(spec)\n",
    "            sensitivity.append(sens)\n",
    "            accuracy.append(metrics.accuracy_score(train_y,data_test['tkeo_label']))\n",
    "            \n",
    "\n",
    "            data_test['tkeo_label'][0]=1\n",
    "            train_y[0]=1\n",
    "            ROC.append(roc_auc_score(train_y,data_test['tkeo_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=pd.DataFrame(file_name)\n",
    "sepcificity=pd.DataFrame(sepcificity)\n",
    "sensitivity=pd.DataFrame(sensitivity)\n",
    "accuracy=pd.DataFrame(accuracy)\n",
    "ROC=pd.DataFrame(ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame=[file_name,sepcificity,sensitivity,accuracy,ROC]\n",
    "performance=pd.concat(frame,axis=1)\n",
    "performance.columns=['file_name','sepcificity','sensitivity','accuracy','ROC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.to_csv('Epi-AI_Model_I_Performance.csv')\n",
    "#performance.to_csv('Epi-AI_Model_II_Performance.csv')\n",
    "#performance.to_csv('Epi-AI_Model_III_Performance.csv')\n",
    "#performance.to_csv('Epi-AI_Model_IV_Performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
